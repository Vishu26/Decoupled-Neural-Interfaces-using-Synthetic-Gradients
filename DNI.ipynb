{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "X, y = mnist.data, mnist.target\n",
    "index = np.random.permutation(70000)\n",
    "X, y = X[index], y[index]\n",
    "\n",
    "x_train, y_train, x_test, y_test = X[:60000], y[:60000], X[60000:], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNI:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.synapticWeights = []\n",
    "        self.syntheticGradients = []\n",
    "        \n",
    "    def add(self, units, input_shape=None):\n",
    "        \n",
    "        if not self.synapticWeights:\n",
    "            if input_shape:\n",
    "                self.synapticWeights.append(np.random.rand(input_shape, units))\n",
    "                self.syntheticGradients.append(np.random.rand(units, input_shape))\n",
    "            return\n",
    "        else:\n",
    "            self.synapticWeights.append(np.random.rand(self.synapticWeights[-1].shape[1], units))\n",
    "            self.syntheticGradients.append(np.random.rand(units, self.syntheticGradients[-1].shape[1]))\n",
    "    \n",
    "    def sigmoid(self, X, deriv=False):\n",
    "        \n",
    "        if not deriv:\n",
    "            return 1/(1+np.exp(-X))\n",
    "        return X*(1-X)\n",
    "    \n",
    "    def fit(self, x, Y, batch_size=32, epochs=10, learning_rate=0.1, synGrad=False):\n",
    "        \n",
    "        #### Forward Prop ####    \n",
    "            \n",
    "        layers = len(self.synapticWeights)\n",
    "        forward = [0]*layers\n",
    "        backward = [0]*layers\n",
    "        \n",
    "        '''if not synGrad:\n",
    "            for i in range(layers-1):\n",
    "                if i==0:\n",
    "                    self.syntheticGradients.append(np.random.rand(self.synapticWeights[0].shape[0], batch_size))\n",
    "                else:\n",
    "                    self.syntheticGradients.append(np.random.rand(self.syntheticGradients[-1].shape[1], batch_size))'''\n",
    "        \n",
    "        num_sGrad = len(self.syntheticGradients) - 1\n",
    "        sGradFor = [0]*num_sGrad\n",
    "        sGradBack = [0]*num_sGrad\n",
    "        \n",
    "        batch = len(x) // batch_size\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            for j in range(batch):\n",
    "                \n",
    "                X = x[j*batch_size:j*batch_size + batch_size]\n",
    "                y = Y[j*batch_size:j*batch_size + batch_size]\n",
    "                \n",
    "                z = np.zeros((64, 10))\n",
    "                for i in range(64):\n",
    "                    z[i][int(y[i])] = 1\n",
    "                y = z\n",
    "                \n",
    "                for i in range(layers):\n",
    "                    if i==0:\n",
    "                        forward[i] = self.sigmoid(X.dot(self.synapticWeights[i]))\n",
    "                    else:\n",
    "                        forward[i] = self.sigmoid(forward[i-1].dot(self.synapticWeights[i]))\n",
    "\n",
    "                for i in range(num_sGrad):\n",
    "                    sGradFor[i] = self.sigmoid(forward[i].dot(self.syntheticGradients[i]))\n",
    "    \n",
    "                if not synGrad:\n",
    "\n",
    "                    #### Back Prop ####\n",
    "\n",
    "                    for i in range(layers-1, -1, -1):\n",
    "\n",
    "                        if i==layers-1:\n",
    "                            error = forward[i] - y\n",
    "                            backward[i] = error*self.sigmoid(forward[i], deriv=True)/2\n",
    "                            if j == batch-1:\n",
    "                                print(\"Loss :\", np.mean(np.square(error)))\n",
    "                        else:\n",
    "                            error = backward[i+1].dot(self.synapticWeights[i+1].T)\n",
    "                            backward[i] = error*self.sigmoid(forward[i], deriv=True)/2\n",
    "\n",
    "                    for i in range(layers):\n",
    "\n",
    "                        if i==0:\n",
    "                            self.synapticWeights[i] = self.synapticWeights[i] - X.T.dot(backward[i]) * learning_rate\n",
    "                        else:\n",
    "                            self.synapticWeights[i] = self.synapticWeights[i] - forward[i-1].T.dot(backward[i]) * learning_rate\n",
    "\n",
    "                    for i in range(num_sGrad):\n",
    "\n",
    "                        if i==0:\n",
    "                            print(backward[i+1].dot(self.synapticWeights[i+1].T).shape)\n",
    "                            error = sGradFor[i] - X.T.dot(backward[i])\n",
    "                            \n",
    "                            sGradBack[i] = error*self.sigmoid(sGradFor[i], deriv=True)/2\n",
    "                            print(sGradBack[i].shape)\n",
    "                            self.syntheticGradients[i] = self.syntheticGradients[i] - sGradFor[i]*sGradBack[i] * learning_rate\n",
    "\n",
    "                        else:\n",
    "                            error = sGradFor[i] - forward[i-1].T.dot(backward[i])\n",
    "                            sGradBack[i] = error*self.sigmoid(sGradFor[i], deriv=True)/2\n",
    "                            self.syntheticGradients[i] = self.syntheticGradients[i] - sGradFor[i]*sGradBack[i] * learning_rate                    \n",
    "\n",
    "                else:\n",
    "\n",
    "                    for i in range(layers-1):\n",
    "\n",
    "                        delta = self.sigmoid(forward[i].dot(self.syntheticGradients[i]))\n",
    "                        self.synapticWeights[i] = self.synapticWeights[i] - delta * learning_rate\n",
    "\n",
    "                    error = forward[layers-1] - y\n",
    "                    d = error*self.sigmoid(forward[layers-1], deriv=True)/2\n",
    "                    self.synapticWeights[layers-1] = self.synapticWeights[layers-1] - forward[layers-2].T.dot(d)* learning_rate\n",
    "\n",
    "    def predict(self, X):\n",
    "        layers = len(self.synapticWeights)\n",
    "        forward = [0]*layers\n",
    "        for i in range(layers):\n",
    "                if i==0:\n",
    "                    forward[i] = self.sigmoid(np.array(X).dot(self.synapticWeights[i]))\n",
    "                else:\n",
    "                    forward[i] = self.sigmoid(forward[i-1].dot(self.synapticWeights[i]))\n",
    "        return forward[layers-1]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_train = x_train.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pretraining ###\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = DNI()\n",
    "model.add(500, input_shape=784)\n",
    "model.add(300)\n",
    "model.add(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
